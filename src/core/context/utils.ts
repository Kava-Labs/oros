import { v4 as uuidv4 } from 'uuid';
import { TextStreamStore } from '../stores/textStreamStore';
import { MessageHistoryStore } from '../stores/messageHistoryStore';
import { ConversationHistory } from './types';
import OpenAI from 'openai';
import { ModelConfig } from '../types/models';
import { isContentChunk } from '../utils/streamUtils';
import { formatConversationTitle } from '../utils/conversation/helpers';
import {
  ChatCompletionChunk,
  ChatCompletionContentPart,
  ChatCompletionMessageParam,
} from 'openai/resources/index';
import { getImage } from '../utils/idb/idb';
import { visionModelPrompt } from '../config/models/defaultPrompts';
import { isReasoningModel } from '../config/models';

export const newConversation = () => {
  return {
    conversationID: uuidv4(),
    messageStore: new TextStreamStore(),
    thinkingStore: new TextStreamStore(),
    progressStore: new TextStreamStore(),
    errorStore: new TextStreamStore(),
    messageHistoryStore: new MessageHistoryStore(),
    isRequesting: false,
  };
};

const analyzeImage = async (
  client: OpenAI,
  msg: ChatCompletionMessageParam,
) => {
  const data = await client.chat.completions.create({
    model: 'qwen2.5-vl-7b-instruct',
    messages: [msg],
  });

  // todo: needs better error handling
  if (!data.choices) {
    return JSON.stringify(data);
  }

  const imageDetails = data.choices[0].message.content;

  return imageDetails ? imageDetails : '';
};

/**
 * Manages a chat interaction with an AI model, handling streaming responses, tool calls,
 * and thinking/reasoning content.
 *
 * @param controller - AbortController to cancel the request if needed
 * @param client - OpenAI client instance for making API requests
 * @param messageHistoryStore - Store containing the conversation history
 * @param modelConfig - Configuration for the AI model including model ID and available tools
 * @param progressStore - Store for tracking and displaying progress status (e.g., "Thinking")
 * @param messageStore - Store for streaming the current message content
 * @param toolCallStreamStore - Store for tracking tool calls generated by the model
 * @param thinkingStore - Store for streaming the model's reasoning/thinking content
 * @param executeOperation - Function to execute tool operations when requested by the model
 * @param conversationID - Unique identifier for the current conversation
 *
 * The function handles different model behaviors, particularly the "deepseek-r1" model
 * which provides explicit thinking content within <think></think> tags.
 *
 * @returns Promise<void> - Resolves when the chat interaction completes or errors
 */
export async function doChat(
  controller: AbortController,
  client: OpenAI,
  messageHistoryStore: MessageHistoryStore,
  modelConfig: ModelConfig,
  progressStore: TextStreamStore,
  messageStore: TextStreamStore,
  thinkingStore: TextStreamStore,
  conversationID: string,
) {
  progressStore.setText('Thinking');
  const { id, tools } = modelConfig;

  const lastMsg =
    messageHistoryStore.getSnapshot()[
      messageHistoryStore.getSnapshot().length - 1
    ];

  let isFileUpload = false;
  const imageIDs: string[] = [];
  let userPromptForImage = '';

  const visionModelMsg: ChatCompletionMessageParam = {
    role: 'user',
    content: [
      {
        type: 'text',
        text: visionModelPrompt,
      },
    ],
  };

  // if the last user message is a file upload
  // take the id out of the image_url and replace it with the base64 imageURL from idb
  if (lastMsg && Array.isArray(lastMsg.content)) {
    for (const content of lastMsg.content) {
      if (content.type === 'image_url') {
        isFileUpload = true;
        progressStore.setText('Analyzing image');
        imageIDs.push(content.image_url.url);
        const img = await getImage(content.image_url.url);
        (visionModelMsg.content as ChatCompletionContentPart[]).push({
          type: 'image_url',
          image_url: {
            url: img ? img.data : 'image not found!',
          },
        });
      }
      if (content.type === 'text') {
        userPromptForImage = content.text;
      }
    }
  }

  if (isFileUpload) {
    const imageDetails = await analyzeImage(client, visionModelMsg);

    messageHistoryStore.setMessages([
      ...messageHistoryStore.getSnapshot().slice(0, -1),
      {
        role: 'system',
        content: JSON.stringify(
          {
            context: 'User Image Uploaded and Analyzed by a vision model',
            imageIDs,
            imageDetails,
          },
          null,
          2,
        ),
      },
      {
        role: 'user',
        content: userPromptForImage,
      },
    ]);
  }

  try {
    // Create a copy of messages without reasoningContent to stream to the model
    // but don't modify the original messages in the store which we will render to the UI
    const messagesWithoutReasoningContent = messageHistoryStore
      .getSnapshot()
      .map((msg) => {
        const msgCopy = { ...msg };
        if ('reasoningContent' in msgCopy) {
          delete msgCopy.reasoningContent;
        }
        return msgCopy;
      });

    let finalChunk: ChatCompletionChunk | undefined;
    const stream = await client.chat.completions.create(
      {
        model: id,
        messages: messagesWithoutReasoningContent,
        tools: tools,
        stream: true,
        stream_options: { include_usage: modelConfig.includeUsageInStream },
      },
      {
        signal: controller.signal,
      },
    );
    let content = '';
    let thinkingEnd = -1;
    let hasCloseTag = false;
    let openTagFound = false;

    for await (const chunk of stream) {
      //  Get the usage info (in the final chunk)
      if (chunk.usage) {
        finalChunk = chunk;
      }

      if (progressStore.getSnapshot() !== '') {
        progressStore.setText('');
      }

      //  todo: chance to clean up into a helper similar to assembleToolCallFromStream
      if (isContentChunk(chunk)) {
        //  Add content from chunks that have it and not the final usage chunk if it exists
        if (chunk.choices.length) {
          content += chunk['choices'][0]['delta']['content'];
        }

        switch (true) {
          case isReasoningModel(modelConfig.id): {
            const openTag = '<think>';
            const closeTag = '</think>';

            if (!openTagFound && content.includes(openTag)) {
              openTagFound = true;
              //  Remove the initial opening tag if present
              content = content.replace(openTag, '');
            }

            if (!hasCloseTag && content.includes(closeTag)) {
              hasCloseTag = true;
              thinkingEnd = content.indexOf(closeTag);
            }

            //  Split the content when the closing tag is found
            if (hasCloseTag) {
              thinkingStore.setText(content.slice(0, thinkingEnd));

              messageStore.setText(
                content.slice(thinkingEnd + closeTag.length),
              );
            } else {
              thinkingStore.setText(content);
            }
            break;
          }

          default: {
            messageStore.setText(content);
            break;
          }
        }
      }
    }

    // Push a message
    if (messageStore.getSnapshot() !== '') {
      const msg = {
        role: 'assistant' as const,
        content: messageStore.getSnapshot(),
      };

      if (thinkingStore.getSnapshot() !== '') {
        // @ts-expect-error setting reasoningContent
        msg.reasoningContent = thinkingStore.getSnapshot();
        thinkingStore.setText('');
      }

      messageHistoryStore.addMessage(msg);

      messageStore.setText('');
    }

    syncWithLocalStorage(
      conversationID,
      modelConfig,
      messageHistoryStore,
      client,
      finalChunk,
    );
  } catch (e) {
    console.error(`An error occurred: ${e} `);
    throw e;
  } finally {
    // Clear progress text if not cleared already
    if (progressStore.getSnapshot() !== '') {
      progressStore.setText('');
    }

    // Ensure content is published on abort
    if (messageStore.getSnapshot() !== '') {
      messageHistoryStore.addMessage({
        role: 'assistant' as const,
        content: messageStore.getSnapshot(),
      });
      messageStore.setText('');
    }
  }
}

/**
 * Synchronizes the current conversation state with local storage, managing conversation
 * history, tracking token usage, and generating conversation titles.
 *
 * @param conversationID - Unique identifier for the conversation to be saved
 * @param modelConfig - Configuration for the AI model including ID and context length
 * @param messageHistoryStore - Store containing the current conversation messages
 * @param client - OpenAI client instance used for generating conversation titles
 * @param finalChunk - Optional completion chunk containing token usage information (used by deepseek models)
 *
 * @returns Promise<void> - Resolves when synchronization is complete
 */
export async function syncWithLocalStorage(
  conversationID: string,
  modelConfig: ModelConfig,
  messageHistoryStore: MessageHistoryStore,
  client: OpenAI,
  finalChunk?: ChatCompletionChunk, // Used by deepseek for token tracking
) {
  const { id, contextLength } = modelConfig;
  const messages = messageHistoryStore.getSnapshot();
  const firstUserMessage = messages.find((msg) => msg.role === 'user');
  if (!firstUserMessage) return;

  const { content } = firstUserMessage;

  const allConversations: Record<string, ConversationHistory> = JSON.parse(
    localStorage.getItem('conversations') ?? '{}',
  );

  const existingConversation = allConversations[conversationID];

  // Initialize tokensRemaining from existing conversation or default to full context length
  let tokensRemaining = existingConversation?.tokensRemaining ?? contextLength;

  //  Deepseek & Qwen compare the current message to the remaining context
  //  GPT compares the entire conversation thread to the max context
  const contextToProcess = isReasoningModel(modelConfig.id)
    ? tokensRemaining
    : contextLength;

  const metrics = await modelConfig.contextLimitMonitor(
    messages,
    contextToProcess,
    finalChunk,
  );
  tokensRemaining = metrics.tokensRemaining;

  const history: ConversationHistory = {
    id: conversationID,
    model: existingConversation ? existingConversation.model : id,
    title: existingConversation ? existingConversation.title : 'New Chat', // initial & fallback value
    conversation: messages,
    lastSaved: new Date().valueOf(),
    tokensRemaining,
  };

  if (existingConversation && existingConversation.conversation.length <= 4) {
    try {
      const data = await client.chat.completions.create({
        stream: false,
        messages: [
          {
            role: 'system',
            content:
              'your task is to generate a title for a conversation using 3 to 4 words',
          },
          {
            role: 'user',
            content: `Please generate a title for this conversation (max 4 words):
                      ${messages.map((msg) => {
                        // only keep user/assistant messages
                        if (msg.role !== 'user' && msg.role !== 'assistant')
                          return;

                        return `Role: ${msg.role} 
                                      ${msg.content}
                        `;
                      })}
                      `,
          },
        ],
        model: 'gpt-4o-mini',
      });

      // Apply truncation only when we get the AI-generated title
      const generatedTitle =
        data.choices[0].message.content ?? (content as string);
      history.title = formatConversationTitle(generatedTitle, 34);
    } catch (err) {
      history.title = content as string;
      console.error(err);
    }
  }

  allConversations[conversationID] = history;
  localStorage.setItem('conversations', JSON.stringify(allConversations));
}
